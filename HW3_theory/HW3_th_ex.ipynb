{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Unsupervised Learning\n",
    "\n",
    "# Part 1: Classification with K-means algorithm\n",
    "\n",
    "The K-means algorithm is a fundamental tool among the unsupervised learning model. Consider a problem with a dataset $\\mathcal{D} = \\left\\{x_i\\right\\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d$ with no labels, we are aiming at finding some hidden structure within the data, namely, we would like to find clusters in the dataset. Classifiers have been studied in TP but mainly for supervised learning, here the data are not labeled. \n",
    "\n",
    "The K-means algorithm tries to classify the dataset in $K$ clusters. Each cluster is represented by a centroid, meaning the average of the points within the cluster. We note $C_k$ the set of points of a cluster $k$ and $\\mu_k$ its centroid. Then, the algorithm minimizes the intra-cluster variance, in other words, it tries to reduce the distance between the points of the cluster and the centroid. \n",
    "\n",
    "More technically, the algorithm works iteratively in two main steps: \n",
    " - Points are assigned to clusters based on their proximity to existing centroid\n",
    " - Centroids are updated by taking the average of the points assigned to each cluster.\n",
    "\n",
    "\n",
    "\n",
    "The Loss function of the problem can be written as:\n",
    "$$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 \\; ,$$\n",
    "where $\\mu(i)$ is the centroid of the cluster assignated to $x_i$. \n",
    "\n",
    "We want to check the understanding of the choice of this loss function. Does it match the aforementioned rules? Then, could it help to understand if the algorithm converges?\n",
    "\n",
    "Additionnally, until Question 8, we assume that the clusters $C_k$ are disjoint, especially at initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "First, let us familiarize ourselves with the loss function:\n",
    "\n",
    " - Prove the second equality:\n",
    "\n",
    "   $$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$$\n",
    "\n",
    " - What does the term $\\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$ represent?\n",
    " \n",
    " - Explain why this form of the loss function is more convenient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 1: K-means loss decomposition\n",
    "\n",
    "- Equality: clusters {C_k} partition the data and each point x_i is paired with its cluster centroid $\\mu(i)=\\mu_k$. Grouping the point-wise sum by cluster merely reorders terms, giving $J=\\sum_i \\|x_i-\\mu(i)\\|^2 = \\sum_{k=1}^K \\sum_{x_i\\in C_k} \\|x_i-\\mu_k\\|^2$.\n",
    "- Meaning: $\\sum_{x_i\\in C_k}\\|x_i-\\mu_k\\|^2$ is the within-cluster sum of squared distances (dispersion/variance) for cluster $k$.\n",
    "- Convenience: the loss separates cluster-wise; centroid updates and assignment changes affect only their cluster term, making the optimization modular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Let us focus on the first point of the algorihtm, let us consider a single point $x_i$ and add the time dependency. Also, we denote by a $'$ the variables after the new assignement of the data points. \n",
    "\n",
    "So that, the variables are denoted by: $(\\cdot)^t \\to (\\cdot)^t\\,{}' \\to (\\cdot)^{t+1} \\to (\\cdot)^{t+1}\\,{}' \\to (\\cdot)^{t+2}$\n",
    "\n",
    "\n",
    "Thus, at each step time $t$, the new assignements of the variables leads to: (no proof required)\n",
    "\n",
    "$$ \\mu^t(i)' = {\\rm argmin}_{\\mu \\in \\left\\{\\mu^t_k\\right\\}_k} \\lVert x_i - \\mu \\rVert^2$$\n",
    "\n",
    "For instance, at time $t$ before new assignements, the vector $x_i$ belongs to a cluster $k$, while after the next assignement, it now belongs to the cluster $k'$ (it can be the same or different from the cluster $k$). \n",
    "\n",
    "\n",
    "Thus, for the whole dataset, the algorithm updates the assignement as: $\\left\\{\\mu^t(i) \\right\\} \\to \\left\\{\\mu^t(i)'\\right\\}$.\n",
    "\n",
    "\n",
    "Compare $\\lVert x_i - \\mu^t(i) \\rVert^2$ and $\\lVert x_i - \\mu^t(i)' \\rVert^2$ for a given point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 2: Assignment step effect\n",
    "\n",
    "By definition of the reassignment step, $\\mu^t(i)'$ is the closest centroid (among $\\{\\mu_k^t\\}\\_k$) to $x_i$. Hence $\\|x_i - \\mu^t(i)'\\|^2 \\le \\|x_i - \\mu^t(i)\\|^2$.\n",
    "- If $x_i$ stays in the same cluster or ties, the distances are equal.\n",
    "- If $x_i$ moves to a new cluster, the distance strictly decreases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "We recall that we denote by a $'$ the variables after the new assignement of the data points, so that: $\\mu^t(i) \\to \\mu^t(i)'$ and $J_t \\to J_t'$\n",
    "\n",
    "Thanks to the previous question, compare $J_t$ and ${J_t}'$. \n",
    "\n",
    "Hint: Pick the right formula between the two given for $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 3: Effect on $J$ after reassignment\n",
    "\n",
    "- Definitions: $J_t = \\sum_i \\|x_i - \\mu^t(i)\\|^2$ and $J_t' = \\sum_i \\|x_i - \\mu^t(i)'\\|^2$.\n",
    "- Point-wise comparison: by construction of the assignment step, $\\mu^t(i)'$ is the closest centroid among $\\{\\mu_k^t\\}$. Therefore $\\|x_i - \\mu^t(i)'\\|^2 \\le \\|x_i - \\mu^t(i)\\|^2$ for every $i$.\n",
    "- Summing over all points: $J_t' = \\sum_i \\|x_i - \\mu^t(i)'\\|^2 \\le \\sum_i \\|x_i - \\mu^t(i)\\|^2 = J_t$.\n",
    "- Equality vs strict decrease: equality holds if no point changes to a strictly closer centroid (e.g., ties or unchanged clusters); otherwise $J_t' < J_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead by studying the second point of the algorithm. It wants to minimize the intra-cluster variance:\n",
    "\n",
    "$$ \\left\\{\\mu^{t+1}k\\right\\}_k = {\\rm argmin}{\\left\\{\\mu_k\\right\\}k \\in \\mathbb{R}^d} J'_t\\left( \\left\\{\\mu_k\\right\\}_k \\right) = {\\rm argmin}{\\left\\{\\mu_k\\right\\}k \\in \\mathbb{R}^d} \\sum{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$\n",
    "\n",
    "\n",
    "Show that the optimization can be done cluster-wise:\n",
    "\n",
    "$$\\mu^{t+1}k = {\\rm argmin}{\\mu_k \\in \\mathbb{R}^d}  \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 4: Cluster-wise optimization setup\n",
    "\n",
    "Goal: minimize $J_t'(\\{\\mu_k\\}_k) = \\sum_{k=1}^K \\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu_k\\|^2$ over centroids $\\{\\mu_k\\}_k$ (assignments fixed).\n",
    "Key observation: the sum is separable by cluster because $\\mu_k$ only appears in the term for $C_k^t{}'$.\n",
    "Rewriting: $J_t' = \\sum_{k=1}^K f_k(\\mu_k)$ with $f_k(\\mu_k) = \\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu_k\\|^2$.\n",
    "Conclusion: we can minimize each $f_k$ independently, yielding $\\mu_k^{t+1} = \\operatorname{argmin}_{\\mu_k} f_k(\\mu_k)$ for every $k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead by studying the second point of the algorithm. It wants to minimize the intra-cluster variance:\n",
    "\n",
    "$$ \\left\\{\\mu^{t+1}_k\\right\\}_k = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} J'_t\\left( \\left\\{\\mu_k\\right\\}_k \\right) = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} \\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$\n",
    "\n",
    "\n",
    "Show that the optimization can be done cluster-wise:\n",
    "\n",
    "$$\\mu^{t+1}_k = {\\rm argmin}_{\\mu_k \\in \\mathbb{R}^d}  \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 5\n",
    "\n",
    "Show that the new centroids of time $t+1$ are computed according to the following equality:\n",
    "\n",
    "$$ \\mu_k^{t+1} = \\frac{1}{|C^t_k{}'|}\\sum_{x_i \\in C^t_k{}'} x_i $$\n",
    "\n",
    "Does it correspond to what you expected from the algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 5: Centroid update\n",
    "\n",
    "We minimize $f_k(\\mu_k) = \\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu_k\\|^2$ for a fixed cluster $C_k^t{}'$.\n",
    "- Expand/gradient: $f_k(\\mu_k) = \\sum_i (x_i^{\\top} x_i - 2 x_i^{\\top} \\mu_k + \\mu_k^{\\top} \\mu_k)$. Gradient: $\\nabla_{\\mu_k} f_k = 2 |C_k^t{}'|\\, \\mu_k - 2 \\sum_{x_i \\in C_k^t{}'} x_i$.\n",
    "- Set to zero: $\\mu_k^{t+1} = \\frac{1}{|C_k^t{}'|} \\sum_{x_i \\in C_k^t{}'} x_i$.\n",
    "- Interpretation: the optimal centroid is the arithmetic mean of its assigned points - exactly the usual K-means update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "\n",
    "If we focus on a cluster $k$ at time $t$ after the assignement, noted $C^t_k {}'$, could you compare $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^t \\rVert^2$ and $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^{t+1} \\rVert^2$ ?\n",
    "\n",
    "What can you say about ${J_t}'$ and $J_{t+1}$ ? Hint: Use the right formula between the two given for $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 6: Improvement from centroid update\n",
    "\n",
    "Consider a fixed cluster $C_k^t{}'$ after the assignment step.\n",
    "- Objective per cluster: $f_k(\\mu) = \\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu\\|^2$. By definition, $\\mu_k^{t+1}$ minimizes $f_k$.\n",
    "- Comparison: $f_k(\\mu_k^{t+1}) \\le f_k(\\mu_k^t)$, i.e., $\\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu_k^{t+1}\\|^2 \\le \\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu_k^t\\|^2$, with equality only if $\\mu_k^t$ was already the minimizer.\n",
    "- Summing over clusters (using the cluster-wise form of $J$): $J_{t+1} = \\sum_k f_k(\\mu_k^{t+1}) \\le \\sum_k f_k(\\mu_k^t) = J_t'$.\n",
    "Thus the centroid update step does not increase the loss; it strictly decreases it unless each old centroid was already the cluster mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7:\n",
    "\n",
    "Putting together the Questions 3 and 5, compare $J_t$ and $J_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 7: Overall decrease across steps\n",
    "\n",
    "- From Q3 (assignment step): $J_t' \\le J_t$, with strict decrease if any point switches to a closer centroid.\n",
    "- From Q6 (centroid minimizer): choosing $\\mu_k^{t+1}$ as the mean of $C_k^t{}'$ minimizes each cluster term, so $J_{t+1} = \\sum_k \\sum_{x_i\\in C_k^t{}'} \\|x_i - \\mu_k^{t+1}\\|^2 \\le J_t'$, with equality only if the old centroids were already the means.\n",
    "- Chain the inequalities: $J_{t+1} \\le J_t' \\le J_t$; each full iteration is non-increasing and strictly decreases $J$ unless neither step changes anything.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8:\n",
    "\n",
    "After recalling a trivial lower bound for the sequence $(J_t)_{t \\geq 0}$, what can you say about the convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 8: Convergence of $J_t$\n",
    "\n",
    "- Lower bound: $J_t = \\sum_i \\|x_i - \\mu^t(i)\\|^2 \\ge 0$ for all $t$.\n",
    "- Monotone: from previous steps, $J_{t+1} \\le J_t$ (non-increasing sequence).\n",
    "- Convergence: a non-increasing sequence bounded below converges, so $J_t \to J^*$ for some $J^* \\ge 0$.\n",
    "- Note on optimality: the limit corresponds to a fixed point of assignments/centroids (a local minimum or stationary point), not necessarily the global minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 9:\n",
    "\n",
    "We just proved that the algorithm converges, but what about its stability:\n",
    "\n",
    "Let us suppose that the data are sampled from a mixture of $K$ Gaussian, where the choice of $K$ is free for this question. Do you imagine a situation where the algorithm does not classify the data at all? Please design and explain the situation as clearly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 9: Example of instability\n",
    "\n",
    "A mixture of Gaussians can produce bad K-means results if clusters overlap heavily or are anisotropic. One clear failure mode: two components that are symmetric and well-separated in one direction but identical along another, leading to multiple equally good partitions.\n",
    "- Example setup: take two Gaussians in $\\mathbb{R}^2$: means $(+m, 0)$ and $(-m, 0)$, same isotropic covariance $\\sigma^2 I$, equal weights. If $m$ is small relative to $\\sigma$, the blobs overlap strongly, and many initializations give different labelings with similar $J$.\n",
    "- Failure to classify: K-means may converge to centroids both sitting near the global mean (collapsed solution) or swap labels arbitrarily; assignments fluctuate with small perturbations, so the algorithm does not recover the true mixture separation.\n",
    "- More severe: with $K$ larger than the true number of modes, centroids can split a single Gaussian into multiple parts instead of separating true components, producing meaningless clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10:\n",
    "\n",
    "What can you say about those configurations of centroids? What does it imply concerning the minima? Conclude your arguments by discussing the convexity of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 10: Minima and non-convexity\n",
    "\n",
    "- Multiple minima: Different centroid configurations can yield the same or similar $J$ (e.g., centroid label swaps; centroids capturing different symmetric partitions). Thesee are distinct local minima.\n",
    "- Saddle/plateaus: With overlapping or symmetric data, centroids can sit at unstable or flat regions (e.g., both centroids at the global mean), so small perturbations change assignments.\n",
    "- Implication: the K-means objective is non-convex in the joint space of assignments and centroids; optimization can get stuck in local minima depending on initialization.\n",
    "- Practical takeaway: multiple restarts or better initializations (k-means++) are used to escape poor local minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11:\n",
    "\n",
    "We can also quickly generalize our algorithm.\n",
    "\n",
    "In some situation, you are aiming at favoring some directions in your data and penalizing the others, so that you can weigh the euclidean distance according to:\n",
    "\n",
    "$$d^{(w)}(x_{i}, \\mu(i)) = \\frac{\\sum_{j=1}^d w_j(x_{ij} - \\mu(i)_j)^2}{\\sum_{j=1}^d w_j} $$\n",
    "\n",
    "Show that with a change of variables, the problem remains the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 11\n",
    "\n",
    "Here the weights $w_j$ (assume $w_j \\ge 0$ and $\\sum_{j=1}^d w_j > 0$) only depend on the coordinate $j$, so they can be absorbed into a rescaling of each coordinate.\n",
    "\n",
    "### Step 1: write the weighted K-means objective\n",
    "\n",
    "If we use the proposed weighted squared-distance, the K-means objective becomes\n",
    "$$\n",
    "J^{(w)}(\\{\\mu_k\\}_{k=1}^K,\\{C_k\\}_{k=1}^K)\n",
    "= \\sum_{k=1}^K \\sum_{x_i\\in C_k} d^{(w)}(x_i,\\mu_k)\n",
    "= \\frac{1}{\\sum_{j=1}^d w_j}\\sum_{k=1}^K \\sum_{x_i\\in C_k} \\sum_{j=1}^d w_j\\,(x_{ij}-\\mu_{kj})^2.\n",
    "$$\n",
    "The denominator $\\sum_j w_j$ is a constant (it does not depend on the clustering nor the centroids), so it does not change which $(\\{C_k\\},\\{\\mu_k\\})$ minimizes the objective.\n",
    "\n",
    "### Step 2: change variables (scale the coordinates)\n",
    "\n",
    "Let $s := \\sum_{j=1}^d w_j$ and define a diagonal scaling matrix\n",
    "$$\n",
    "D := \\operatorname{diag}\\!\\left(\\sqrt{\\frac{w_1}{s}},\\dots,\\sqrt{\\frac{w_d}{s}}\\right).\n",
    "$$\n",
    "Define transformed data points and transformed centroids by\n",
    "$$\n",
    "\\tilde{x}_i := D x_i,\\qquad \\tilde{\\mu}_k := D\\mu_k.\n",
    "$$\n",
    "In coordinates, this is $\\tilde{x}_{ij} = \\sqrt{\\frac{w_j}{s}}\\,x_{ij}$ and $\\tilde{\\mu}_{kj} = \\sqrt{\\frac{w_j}{s}}\\,\\mu_{kj}$.\n",
    "\n",
    "### Step 3: the weighted distance becomes ordinary squared Euclidean distance\n",
    "\n",
    "Compute the squared Euclidean distance in the transformed space:\n",
    "$$\n",
    "\\|\\tilde{x}_i-\\tilde{\\mu}_k\\|^2\n",
    "= \\sum_{j=1}^d \\left(\\sqrt{\\frac{w_j}{s}}(x_{ij}-\\mu_{kj})\\right)^2\n",
    "= \\frac{1}{s}\\sum_{j=1}^d w_j (x_{ij}-\\mu_{kj})^2\n",
    "= d^{(w)}(x_i,\\mu_k).\n",
    "$$\n",
    "So the â€œweightedâ€ distance is exactly the usual squared Euclidean distance after the change of variables.\n",
    "\n",
    "### Step 4: conclude equivalence of problems\n",
    "\n",
    "Plugging the identity above into the objective gives\n",
    "$$\n",
    "J^{(w)}(\\{\\mu_k\\},\\{C_k\\}) = \\sum_{k=1}^K \\sum_{x_i\\in C_k} \\|\\tilde{x}_i-\\tilde{\\mu}_k\\|^2,\n",
    "$$\n",
    "which is exactly the standard K-means objective applied to the transformed points $\\{\\tilde{x}_i\\}$.\n",
    "\n",
    "Therefore the algorithm/problem â€œremains the sameâ€ in the following precise sense:\n",
    "- Assignment step: $\\arg\\min_k d^{(w)}(x_i,\\mu_k) = \\arg\\min_k \\|\\tilde{x}_i-\\tilde{\\mu}_k\\|^2$.\n",
    "- Centroid update: in transformed space, $\\tilde{\\mu}_k$ is the average of the $\\tilde{x}_i$ in cluster $k$; if all $w_j>0$ (so $D$ is invertible), mapping back gives $\\mu_{kj} = \\frac{1}{|C_k|}\\sum_{x_i\\in C_k} x_{ij}$.\n",
    "\n",
    "Edge case: if some $w_j=0$, that coordinate is ignored by the distance; it does not affect the objective, so any value of $\\mu_{kj}$ along that coordinate yields the same loss (one typically still uses the mean)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Restricted Boltzmann Machine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The Boltzmann Machine have been inspired by thermodynamic and statistical physics models, more precisely they are part of the Energy Models using the well known Boltzmann Distribution as written in physics style:\n",
    "\n",
    "$$ P\\left( E \\right)  = \\frac{1}{Z} \\exp \\left( -\\frac{E}{k_b T} \\right)$$\n",
    "\n",
    "It becomes in statistical inference framework:\n",
    "$$\n",
    "P(\\mathbf{v} | J, \\mathbf{b}) \\propto e^{\\mathbf{v}^TJ\\mathbf{v} + \\mathbf{b}^T\\mathbf{v}} = e^{-E(\\mathbf{v})}\n",
    "$$\n",
    "where:\n",
    "- $\\mathbf{v}\\in\\mathbb{R}^n:$ The binary vector with components $v_i = 0 \\; {\\rm or} \\; 1$\n",
    "\n",
    "- $J \\in \\mathbb{R}^{n \\times n}:$ The coupling matrix\n",
    "\n",
    "- $\\mathbf{b} \\in  \\mathbb{R}^n$: Field\n",
    "\n",
    "- $E(\\mathbf{v}) \\in  \\mathbb{R}$: Energy\n",
    "\n",
    "\n",
    "However, one problem arised with initial Boltzmann Machine (BM) -- like its parent models in statistical physics (as the SK model) -- all the units are interacting through complicated dependencies. For example, if we consider 3 components of $\\mathbf{v}$: $v_1$, $v_2$, and $v_3$, there are trivial interactions such as one modelised by $P(v_1, v_2)$ corresponding to the correlation between the two first components of $\\mathbf{v}$, but there are also none trivial interactions. Indeed, if some term like $P(v_1, v_2 | v_3)$ which suggests that the correlation $x_1$ and $v_2$ depends on $v_3$ and this is clearly none linear.\n",
    "\n",
    "A really ingenious way to overcome this situation is to replace all the tricky interactions between the units $\\mathbf{v}\\in\\mathbb{R}^n$ by connections through hidden units $\\mathbf{h}\\in\\mathbb{R}^m$, artifically created. Indeed, correlations between two units $v_1$ and $v_2$ (specially the dependency of their correlations on other units $v_3$, $v_4$,...) can be atrificially replaced by introducing a third unit $h_1$ and considerin only linear correlations between $v_1 \\leftrightarrow h_1$, $h_1 \\leftrightarrow v_2$ and $v_1 \\leftrightarrow v_2$. The units $v_i$ are now called the visible units. This model is the most known version of BMs. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"boltzmannmachine.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "However, this model is still fully connected and makes the computation really costful. Then, one can even simplify the model by considering zero intra layer interractions. This simplified model is call Restricted Boltzmann Machine (RBM) (Physics Nobel Price 2024 ðŸ¥³).\n",
    "\n",
    "Thus, the RBM architecture consists of two layers of binary stochastic units: a $\\textbf{visible layer}$ $\\mathbf{v}$ and a $\\textbf{hidden layer}$ $\\mathbf{h}$. The layers are fully connected, but there are no connections within a layer, making the model a $\\textbf{bipartite graph}$. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"rbm.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "Restricted Boltzmann Machines (RBMs) are a class of energy-based probabilistic graphical models that are commonly used in machine learning for tasks such as dimensionality reduction, feature learning, and generative modeling.\n",
    "\n",
    "### Energy Function and Probabilities\n",
    "\n",
    "The joint configuration of the visible units $\\mathbf{v} \\in \\{0, 1\\}^d$ and the hidden units $\\mathbf{h} \\in \\{0, 1\\}^m$ is associated with an $\\textbf{energy function}$, defined as:\n",
    "\n",
    "$$ E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{v}^\\top \\mathbf{W} \\mathbf{h} - \\mathbf{b}^\\top \\mathbf{v} - \\mathbf{c}^\\top \\mathbf{h}$$\n",
    "where:\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{d \\times m}$ is the weight matrix connecting the visible and hidden units,\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^d$ field of the visible units or also called the biases of the visible units,\n",
    "- $\\mathbf{c} \\in \\mathbb{R}^m$ field of the hidden units of also called the biases of the hidden units.\n",
    "\n",
    "The energy function determines the joint probability distribution over $\\mathbf{v}$ and $\\mathbf{h}$:\n",
    "$$ P(\\mathbf{v}, \\mathbf{h}) = \\frac{1}{Z} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "where $Z$ is the $\\textbf{partition function}$, ensuring normalization:\n",
    "\n",
    "$$ Z = \\sum_{\\mathbf{v}, \\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "\n",
    "\n",
    "The marginal probability of the visible units $\\mathbf{v}$ is obtained by summing over all possible configurations of the hidden units:\n",
    "\n",
    "$$ P(\\mathbf{v}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12:\n",
    "\n",
    "Write a valid expression of the energy $E(\\textbf{v}, \\textbf{h})$ in the case of a BM (non-restricted) with an hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 12: Energy of a general Boltzmann Machine with a hidden layer\n",
    "\n",
    "Variables:\n",
    "- Visible units: $v \\in \\{0,1\\}^d$\n",
    "- Hidden units: $h \\in \\{0,1\\}^m$\n",
    "\n",
    "Parameters:\n",
    "- Visible biases: $b \\in \\mathbb{R}^d$\n",
    "- Hidden biases: $c \\in \\mathbb{R}^m$\n",
    "- Visibleâ€“visible weights: $L \\in \\mathbb{R}^{d \\times d}$, symmetric with zero diagonal\n",
    "- Hiddenâ€“hidden weights: $K \\in \\mathbb{R}^{m \\times m}$, symmetric with zero diagonal\n",
    "- Visibleâ€“hidden weights: $W \\in \\mathbb{R}^{d \\times m}$\n",
    "\n",
    "A valid energy:\n",
    "$$\n",
    "E(v,h) = -\\,b^\\top v \\;-\\; c^\\top h \\;-\\; \\tfrac{1}{2}\\,v^\\top L\\,v \\;-\\; v^\\top W\\,h \\;-\\; \\tfrac{1}{2}\\,h^\\top K\\,h \\, .\n",
    "$$\n",
    "The $\\tfrac{1}{2}$ factors avoid double counting symmetric pairs in $L$ and $K$ and the zero diagonals avoid self-interactions.\n",
    "\n",
    "Block-matrix form:\n",
    "Define $x = \\begin{bmatrix} v \\\\ h \\end{bmatrix}$ and $M = \\begin{bmatrix} L & W \\\\ W^\\top & K \\end{bmatrix}$ (symmetric, zero diagonal). Then\n",
    "$$\n",
    "E(x) = -\\,\\tfrac{1}{2}\\,x^\\top M x \\;-\\; \\begin{bmatrix} b \\\\ c \\end{bmatrix}^\\top x \\, .\n",
    "$$\n",
    "Either expression is a correct energy for a non-restricted Boltzmann Machine with one hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13:\n",
    "\n",
    "One of the key properties of RBMs is the $\\textbf{conditional independence}$ between units within a layer:\n",
    "\n",
    "Compute the conditional probability and show that:\n",
    "\n",
    "$$ P(h_j = 1 | \\mathbf{v}) = \\sigma\\left(c_j + \\sum_{i} v_i W_{ij}\\right) $$\n",
    "and\n",
    "$$ P(v_i = 1 | \\mathbf{h}) = \\sigma\\left(b_i + \\sum_{j} h_j W_{ij}\\right) $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid activation function.\n",
    "\n",
    "This bipartite structure enables efficient Gibbs sampling for approximating the intractable joint distribution $P(\\mathbf{v}, \\mathbf{h})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 13:\n",
    "\n",
    "We start from the RBM energy (no within-layer connections):\n",
    "$$\n",
    "E(\\mathbf{v},\\mathbf{h}) = -\\mathbf{v}^\\top \\mathbf{W}\\mathbf{h} - \\mathbf{b}^\\top \\mathbf{v} - \\mathbf{c}^\\top \\mathbf{h},\n",
    "$$\n",
    "so the joint distribution is\n",
    "$$\n",
    "P(\\mathbf{v},\\mathbf{h}) = \\frac{1}{Z}\\exp\\big(-E(\\mathbf{v},\\mathbf{h})\\big)\n",
    "= \\frac{1}{Z}\\exp\\big(\\mathbf{v}^\\top \\mathbf{W}\\mathbf{h} + \\mathbf{b}^\\top \\mathbf{v} + \\mathbf{c}^\\top \\mathbf{h}\\big).\n",
    "$$\n",
    "\n",
    "The key structural fact is: the graph is bipartite (no $v_i\\text{--}v_{i'}$ edges and no $h_j\\text{--}h_{j'}$ edges). This is exactly what will make the conditionals factorize.\n",
    "\n",
    "## 1) Compute $P(h_j=1\\mid \\mathbf{v})$ and show conditional independence in the hidden layer\n",
    "\n",
    "By definition,\n",
    "$$\n",
    "P(\\mathbf{h}\\mid\\mathbf{v}) = \\frac{P(\\mathbf{v},\\mathbf{h})}{\\sum_{\\mathbf{h}'}P(\\mathbf{v},\\mathbf{h}') }.\n",
    "$$\n",
    "When $\\mathbf{v}$ is fixed, the term $\\mathbf{b}^\\top\\mathbf{v}$ is a constant (it does not depend on $\\mathbf{h}$), so it cancels out in the normalization. Therefore we only keep the terms that depend on $\\mathbf{h}$:\n",
    "$$\n",
    "P(\\mathbf{h}\\mid\\mathbf{v}) \\propto \\exp\\big(\\mathbf{v}^\\top \\mathbf{W}\\mathbf{h} + \\mathbf{c}^\\top \\mathbf{h}\\big).\n",
    "$$\n",
    "Rewrite the exponent as a sum over coordinates of $\\mathbf{h}$:\n",
    "$$\n",
    "\\mathbf{v}^\\top \\mathbf{W}\\mathbf{h} + \\mathbf{c}^\\top \\mathbf{h}\n",
    "= \\sum_{j=1}^m h_j\\left(c_j + \\sum_{i=1}^d v_i W_{ij}\\right).\n",
    "$$\n",
    "Define\n",
    "$$\n",
    "a_j(\\mathbf{v}) := c_j + \\sum_{i=1}^d v_i W_{ij}.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "P(\\mathbf{h}\\mid\\mathbf{v}) \\propto \\exp\\left(\\sum_{j=1}^m h_j a_j(\\mathbf{v})\\right)\n",
    "= \\prod_{j=1}^m \\exp\\big(h_j a_j(\\mathbf{v})\\big).\n",
    "$$\n",
    "This product form shows **conditional independence**:\n",
    "$$\n",
    "P(\\mathbf{h}\\mid\\mathbf{v}) = \\prod_{j=1}^m P(h_j\\mid\\mathbf{v}).\n",
    "$$\n",
    "\n",
    "Now compute each factor. Since $h_j\\in\\{0,1\\}$,\n",
    "- if $h_j=0$ then $\\exp(h_j a_j)=1$,\n",
    "- if $h_j=1$ then $\\exp(h_j a_j)=\\exp(a_j)$.\n",
    "So, after normalization over $h_j\\in\\{0,1\\}$,\n",
    "$$\n",
    "P(h_j=1\\mid\\mathbf{v}) = \\frac{e^{a_j(\\mathbf{v})}}{1+e^{a_j(\\mathbf{v})}} = \\sigma\\big(a_j(\\mathbf{v})\\big)\n",
    "= \\sigma\\left(c_j + \\sum_{i=1}^d v_i W_{ij}\\right).\n",
    "$$\n",
    "Equivalently, $P(h_j=0\\mid\\mathbf{v}) = 1 - P(h_j=1\\mid\\mathbf{v})$.\n",
    "\n",
    "## 2) Compute $P(v_i=1\\mid \\mathbf{h})$ and show conditional independence in the visible layer\n",
    "\n",
    "The derivation is symmetric. By definition,\n",
    "$$\n",
    "P(\\mathbf{v}\\mid\\mathbf{h}) = \\frac{P(\\mathbf{v},\\mathbf{h})}{\\sum_{\\mathbf{v}'}P(\\mathbf{v}',\\mathbf{h})}.\n",
    "$$\n",
    "With $\\mathbf{h}$ fixed, the term $\\mathbf{c}^\\top\\mathbf{h}$ is constant and cancels in the normalization, so\n",
    "$$\n",
    "P(\\mathbf{v}\\mid\\mathbf{h}) \\propto \\exp\\big(\\mathbf{v}^\\top \\mathbf{W}\\mathbf{h} + \\mathbf{b}^\\top \\mathbf{v}\\big)\n",
    "= \\exp\\left(\\sum_{i=1}^d v_i\\left(b_i + \\sum_{j=1}^m h_j W_{ij}\\right)\\right).\n",
    "$$\n",
    "Define $d_i(\\mathbf{h}) := b_i + \\sum_{j=1}^m h_j W_{ij}$. Then\n",
    "$$\n",
    "P(\\mathbf{v}\\mid\\mathbf{h}) \\propto \\prod_{i=1}^d \\exp\\big(v_i d_i(\\mathbf{h})\\big),\n",
    "$$\n",
    "which implies conditional independence:\n",
    "$$\n",
    "P(\\mathbf{v}\\mid\\mathbf{h}) = \\prod_{i=1}^d P(v_i\\mid\\mathbf{h}).\n",
    "$$\n",
    "Normalizing each $v_i\\in\\{0,1\\}$ gives\n",
    "$$\n",
    "P(v_i=1\\mid\\mathbf{h}) = \\frac{e^{d_i(\\mathbf{h})}}{1+e^{d_i(\\mathbf{h})}} = \\sigma\\big(d_i(\\mathbf{h})\\big)\n",
    "= \\sigma\\left(b_i + \\sum_{j=1}^m h_j W_{ij}\\right).\n",
    "$$\n",
    "\n",
    "Final takeaway: because the RBM has **no within-layer interactions**, conditioning on one layer turns the other layer into independent Bernoulli variables with sigmoid parameters. This is why we can sample all $h_j$ in parallel given $\\mathbf{v}$ (and all $v_i$ in parallel given $\\mathbf{h}$) in block Gibbs sampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in RBMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14:\n",
    "\n",
    "Training an RBM involves maximizing the likelihood of the data distribution. To do so we are aiming at using a gradient descent/ascent on the weights (and biases).\n",
    "\n",
    "Compute the log-likelihood $\\mathcal{L}(\\mathbf{v})$, remember that the model is part of the unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 14: Log-likelihood of an RBM\n",
    "\n",
    "Setup:\n",
    "- Visible binary vector: $v \\in \\{0,1\\}^d$\n",
    "- Hidden binary vector: $h \\in \\{0,1\\}^m$\n",
    "- Parameters: weights $W \\in \\mathbb{R}^{d \\times m}$, biases $b \\in \\mathbb{R}^d$ (visible), $c \\in \\mathbb{R}^m$ (hidden)\n",
    "- Energy: $E(v,h) = -\\,v^\\top W h \\;-\\; b^\\top v \\;-\\; c^\\top h$\n",
    "\n",
    "1) Joint and marginal\n",
    "The joint is $P(v,h) = \\frac{1}{Z} \\exp(-E(v,h))$ with partition function $Z = \\sum_{v,h} \\exp(-E(v,h))$.\n",
    "The marginal over $v$ is $p(v) = \\sum_h P(v,h) = \\frac{1}{Z} \\sum_h \\exp(-E(v,h))$.\n",
    "\n",
    "2) Factor the sum over $h$\n",
    "Plugging $E(v,h)$:\n",
    "$$\n",
    "p(v) = \\frac{1}{Z} \\exp(b^\\top v) \\sum_h \\exp\\big( h^\\top (W^\\top v + c) \\big)\n",
    "     = \\frac{1}{Z} \\exp(b^\\top v) \\prod_{j=1}^m \\sum_{h_j \\in \\{0,1\\}} \\exp\\big( h_j (c_j + (W^\\top v)_j) \\big).\n",
    "$$\n",
    "Each inner sum is $1 + \\exp(c_j + (W^\\top v)_j)$.\n",
    "\n",
    "3) Closed form for $p(v)$\n",
    "$$\n",
    "p(v) = \\frac{1}{Z} \\exp(b^\\top v) \\prod_{j=1}^m \\big(1 + e^{\\,c_j + (W^\\top v)_j}\\big).\n",
    "$$\n",
    "\n",
    "4) Log-likelihood\n",
    "$$\n",
    "\\log p(v) = b^\\top v \\;+\\; \\sum_{j=1}^m \\log\\big(1 + e^{\\,c_j + \\sum_i v_i W_{ij}}\\big) \\;-\\; \\log Z.\n",
    "$$\n",
    "\n",
    "5) Free-energy form (optional)\n",
    "Define the free energy $F(v) = -\\,b^\\top v \\;-\\; \\sum_j \\log\\big(1 + e^{\\,c_j + (W^\\top v)_j}\\big)$.\n",
    "Then $\\log p(v) = -F(v) - \\log Z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15:\n",
    "\n",
    "Compute the gradient of the log-likelihood with respect to the weights $\\mathbf{W}$ and the biases $\\mathbf{b}$, $\\mathbf{c}$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it should be possible to implement the RBM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 15: Gradients of the RBM log-likelihood\n",
    "\n",
    "We derive the gradients of the log-likelihood with respect to $W$, $b$, $c$ starting from the definition of the RBM likelihood.\n",
    "\n",
    "#### 1) Definitions\n",
    "\n",
    "Energy (binary-binary RBM):\n",
    "$$\n",
    "E(v,h) = - v^\\top W h - b^\\top v - c^\\top h.\n",
    "$$\n",
    "\n",
    "Joint distribution:\n",
    "$$\n",
    "p(v,h) = \\frac{1}{Z}\\,e^{-E(v,h)},\n",
    "\\qquad\n",
    "Z = \\sum_{v'}\\sum_h e^{-E(v',h)}.\n",
    "$$\n",
    "\n",
    "Marginal likelihood of one visible vector $v$:\n",
    "$$\n",
    "p(v) = \\sum_h p(v,h) = \\frac{1}{Z}\\sum_h e^{-E(v,h)}.\n",
    "$$\n",
    "\n",
    "Log-likelihood of one datum:\n",
    "$$\n",
    "\\ell(v) = \\log p(v) = \\log\\Big(\\sum_h e^{-E(v,h)}\\Big) - \\log Z.\n",
    "$$\n",
    "\n",
    "We compute $\\partial\\ell(v)/\\partial\\theta$ for a generic parameter $\\theta\\in\\{W_{ij},b_i,c_j\\}$, then sum over data points.\n",
    "\n",
    "#### 2) Differentiate the first term $\\log\\big(\\sum_h e^{-E(v,h)}\\big)$\n",
    "\n",
    "Define\n",
    "$$\n",
    "A(v) := \\sum_h e^{-E(v,h)}.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta}\\log A(v)\n",
    "= \\frac{1}{A(v)}\\frac{\\partial A(v)}{\\partial\\theta}.\n",
    "$$\n",
    "\n",
    "Differentiate $A(v)$ by moving the derivative inside the finite sum:\n",
    "$$\n",
    "\\frac{\\partial A(v)}{\\partial\\theta}\n",
    "= \\frac{\\partial}{\\partial\\theta}\\sum_h e^{-E(v,h)}\n",
    "= \\sum_h \\frac{\\partial}{\\partial\\theta} e^{-E(v,h)}.\n",
    "$$\n",
    "Now apply the chain rule to the exponential:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta} e^{-E(v,h)}\n",
    "= e^{-E(v,h)}\\,\\frac{\\partial}{\\partial\\theta}(-E(v,h))\n",
    "= e^{-E(v,h)}\\,\\Big(-\\frac{\\partial E(v,h)}{\\partial\\theta}\\Big).\n",
    "$$\n",
    "Substitute this back into the sum:\n",
    "$$\n",
    "\\frac{\\partial A(v)}{\\partial\\theta}\n",
    "= \\sum_h e^{-E(v,h)}\\,\\Big(-\\frac{\\partial E(v,h)}{\\partial\\theta}\\Big).\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta}\\log A(v)\n",
    "= \\frac{1}{A(v)}\\sum_h e^{-E(v,h)}\\,\\Big(-\\partial_\\theta E(v,h)\\Big)\n",
    "= \\frac{\\sum_h e^{-E(v,h)}\\,\\big(-\\partial_\\theta E(v,h)\\big)}{\\sum_{h'} e^{-E(v,h')}}.\n",
    "$$\n",
    "\n",
    "Recognize the conditional distribution (definition):\n",
    "$$\n",
    "p(h\\mid v) := \\frac{p(v,h)}{p(v)}\n",
    "= \\frac{\\frac{1}{Z}e^{-E(v,h)}}{\\frac{1}{Z}\\sum_{h'}e^{-E(v,h')}}\n",
    "= \\frac{e^{-E(v,h)}}{\\sum_{h'}e^{-E(v,h')}}.\n",
    "$$\n",
    "Therefore\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta}\\log\\Big(\\sum_h e^{-E(v,h)}\\Big)\n",
    "= \\sum_h p(h\\mid v)\\,\\big(-\\partial_\\theta E(v,h)\\big)\n",
    "= \\mathbb{E}_{p(h\\mid v)}\\big[-\\partial_\\theta E(v,h)\\big].\n",
    "$$\n",
    "\n",
    "#### 3) Differentiate the second term $\\log Z$\n",
    "\n",
    "Start from the chain rule again:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta}\\log Z = \\frac{1}{Z}\\frac{\\partial Z}{\\partial\\theta}.\n",
    "$$\n",
    "Differentiate the partition function:\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial\\theta}\n",
    "= \\frac{\\partial}{\\partial\\theta}\\sum_{v'}\\sum_h e^{-E(v',h)}\n",
    "= \\sum_{v'}\\sum_h \\frac{\\partial}{\\partial\\theta} e^{-E(v',h)}\n",
    "= \\sum_{v'}\\sum_h e^{-E(v',h)}\\,\\Big(-\\partial_\\theta E(v',h)\\Big).\n",
    "$$\n",
    "So\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta}\\log Z\n",
    "= \\sum_{v'}\\sum_h \\frac{e^{-E(v',h)}}{Z}\\,\\Big(-\\partial_\\theta E(v',h)\\Big).\n",
    "$$\n",
    "Recognize the model joint distribution $p(v,h)=\\frac{1}{Z}e^{-E(v,h)}$, hence\n",
    "$$\n",
    "\\frac{\\partial}{\\partial\\theta}\\log Z\n",
    "= \\mathbb{E}_{p(v,h)}\\big[-\\partial_\\theta E(v,h)\\big].\n",
    "$$\n",
    "\n",
    "#### 4) Combine both derivatives (one datum)\n",
    "\n",
    "From $\\ell(v)=\\log(\\sum_h e^{-E(v,h)})-\\log Z$:\n",
    "$$\n",
    "\\frac{\\partial\\ell(v)}{\\partial\\theta}\n",
    "= \\mathbb{E}_{p(h\\mid v)}\\big[-\\partial_\\theta E(v,h)\\big]\n",
    "- \\mathbb{E}_{p(v,h)}\\big[-\\partial_\\theta E(v,h)\\big].\n",
    "$$\n",
    "\n",
    "#### 5) Compute $-\\partial_\\theta E$ for $W_{ij}$, $b_i$, $c_j$\n",
    "\n",
    "Expand the energy as scalars:\n",
    "$$\n",
    "E(v,h) = -\\sum_{i=1}^d\\sum_{j=1}^m v_i W_{ij} h_j - \\sum_{i=1}^d b_i v_i - \\sum_{j=1}^m c_j h_j.\n",
    "$$\n",
    "\n",
    "- For $W_{ij}$: only the term $-v_i W_{ij} h_j$ depends on $W_{ij}$, so\n",
    "$$\n",
    "\\frac{\\partial E(v,h)}{\\partial W_{ij}} = -v_i h_j\n",
    "\\quad\\Rightarrow\\quad\n",
    "-\\frac{\\partial E(v,h)}{\\partial W_{ij}} = v_i h_j.\n",
    "$$\n",
    "\n",
    "- For $b_i$: only the term $-b_i v_i$ depends on $b_i$, so\n",
    "$$\n",
    "\\frac{\\partial E(v,h)}{\\partial b_i} = -v_i\n",
    "\\quad\\Rightarrow\\quad\n",
    "-\\frac{\\partial E(v,h)}{\\partial b_i} = v_i.\n",
    "$$\n",
    "\n",
    "- For $c_j$: only the term $-c_j h_j$ depends on $c_j$, so\n",
    "$$\n",
    "\\frac{\\partial E(v,h)}{\\partial c_j} = -h_j\n",
    "\\quad\\Rightarrow\\quad\n",
    "-\\frac{\\partial E(v,h)}{\\partial c_j} = h_j.\n",
    "$$\n",
    "\n",
    "Substitute each case into the generic gradient:\n",
    "\n",
    "**Weights**\n",
    "$$\n",
    "\\frac{\\partial\\ell(v)}{\\partial W_{ij}}\n",
    "= \\mathbb{E}_{p(h\\mid v)}[v_i h_j] - \\mathbb{E}_{p(v,h)}[v_i h_j].\n",
    "$$\n",
    "Because $v$ is fixed inside $p(h\\mid v)$, you can pull $v_i$ out of the expectation:\n",
    "$$\n",
    "\\mathbb{E}_{p(h\\mid v)}[v_i h_j] = v_i\\,\\mathbb{E}_{p(h\\mid v)}[h_j].\n",
    "$$\n",
    "\n",
    "**Visible biases**\n",
    "$$\n",
    "\\frac{\\partial\\ell(v)}{\\partial b_i}\n",
    "= \\mathbb{E}_{p(h\\mid v)}[v_i] - \\mathbb{E}_{p(v,h)}[v_i]\n",
    "= v_i - \\mathbb{E}_{p(v,h)}[v_i].\n",
    "$$\n",
    "\n",
    "**Hidden biases**\n",
    "$$\n",
    "\\frac{\\partial\\ell(v)}{\\partial c_j}\n",
    "= \\mathbb{E}_{p(h\\mid v)}[h_j] - \\mathbb{E}_{p(v,h)}[h_j].\n",
    "$$\n",
    "\n",
    "#### 6) Dataset form\n",
    "\n",
    "If $\\mathcal{L}=\\sum_{n=1}^N \\ell(v^{(n)})$, then by linearity of differentiation:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\theta} = \\sum_{n=1}^N \\frac{\\partial\\ell(v^{(n)})}{\\partial\\theta}.\n",
    "$$\n",
    "So, for example:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W_{ij}}\n",
    "= \\sum_{n=1}^N \\mathbb{E}_{p(h\\mid v^{(n)})}[v_i^{(n)} h_j]\n",
    "- N\\,\\mathbb{E}_{p(v,h)}[v_i h_j],\n",
    "$$\n",
    "and analogously for $b_i$ and $c_j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16: (Open question)\n",
    "\n",
    "While it seems possible to run RBM algorithm, note that the second term in the gradient w.r.t. $\\mathbf{W}$ is computationally expensive due to the intractability of $Z$, the approximation Contrastive Divergence - k is often use. Research what is this approximation, is this approximation enough, why? Explain it with your own words and cite the papers you used for your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 16: Contrastive Divergence (CD-k)\n",
    "\n",
    "Recall from Q15 that for any parameter $\\theta$ the exact gradient of the log-likelihood (for one datum $v$) can be written as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log p(v)}{\\partial \\theta}\n",
    "= \\underbrace{\\mathbb{E}_{p(h\\mid v)}\\big[-\\partial_{\\theta}E(v,h)\\big]}_{\\text{positive phase}}\n",
    "- \\underbrace{\\mathbb{E}_{p(v,h)}\\big[-\\partial_{\\theta}E(v,h)\\big]}_{\\text{negative phase}}.\n",
    "$$\n",
    "\n",
    "The positive phase is tractable in an RBM because $p(h\\mid v)$ factorizes across hidden units. The negative phase is expensive because it requires expectations under the model distribution $p(v,h)$ (and in general depends on the partition function $Z$).\n",
    "\n",
    "### What CD-k does (the approximation)\n",
    "\n",
    "Contrastive Divergence (CD-k) approximates the negative phase by running only $k$ steps of block Gibbs sampling, starting the Markov chain at the data instead of waiting for it to mix all the way to equilibrium.\n",
    "\n",
    "### CD-k update (one mini-batch)\n",
    "\n",
    "For each training example (or mini-batch):\n",
    "1) Initialize $v^{(0)} = v_{\\text{data}}$.\n",
    "2) Sample $h^{(0)} \\sim p(h\\mid v^{(0)})$.\n",
    "3) For $t = 0,\\dots,k-1$:\n",
    "   - Sample $v^{(t+1)} \\sim p(v\\mid h^{(t)})$.\n",
    "   - Sample $h^{(t+1)} \\sim p(h\\mid v^{(t+1)})$.\n",
    "4) Use the endpoints $(v^{(0)},h^{(0)})$ and $(v^{(k)},h^{(k)})$ to form the contrastive update:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta W &\\propto v^{(0)}(h^{(0)})^\\top - v^{(k)}(h^{(k)})^\\top, \\\\\n",
    "\\Delta b &\\propto v^{(0)} - v^{(k)}, \\\\\n",
    "\\Delta c &\\propto h^{(0)} - h^{(k)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "(These quantities are averaged over the mini-batch.)\n",
    "\n",
    "### Why this makes sense\n",
    "\n",
    "- If $k$ were very large and the Gibbs chain mixed perfectly, $(v^{(k)},h^{(k)})$ would be (approximately) distributed as the model $p(v,h)$, so the update would match the true maximum-likelihood gradient from Q15.\n",
    "- For finite $k$, CD-k is biased (it is not the exact log-likelihood gradient). Hinton (2002) shows CD-k can be interpreted as minimizing the contrastive divergence objective\n",
    "\n",
    "$$\n",
    "\\mathrm{CD}_k(\\theta) = \\mathrm{KL}(q_0 \\,\\|\\, p_{\\theta}) - \\mathrm{KL}(q_k \\,\\|\\, p_{\\theta}),\n",
    "$$\n",
    "\n",
    "where $q_0$ is the data distribution and $q_k$ is the distribution obtained after running $k$ Gibbs steps starting from data samples.\n",
    "\n",
    "### Is CD-k enough?\n",
    "\n",
    "- Often good enough for feature learning: small $k$ (commonly 1-10, sometimes 1) is fast and tends to improve the model.\n",
    "- Not always enough for accurate generative modeling: if the Markov chain mixes slowly, a few steps do not approximate $p(v,h)$ well, so likelihood and sample quality can suffer.\n",
    "- A common improvement is Persistent Contrastive Divergence (PCD), also called Stochastic Maximum Likelihood (SML): maintain persistent chains across updates (do not restart at data every time) and run a few Gibbs steps per iteration to approximate the model expectation better.\n",
    "\n",
    "### References\n",
    "\n",
    "- G. E. Hinton, \"Training products of experts by minimizing contrastive divergence\", Neural Computation, 14(8), 2002.\n",
    "- T. Tieleman, \"Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient\", ICML 2008. (Persistent CD)\n",
    "- G. E. Hinton, \"A Practical Guide to Training Restricted Boltzmann Machines\", Technical Report, 2010.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of RBMs\n",
    "\n",
    "RBMs are widely used in tasks such as:\n",
    "\n",
    "- $\\textbf{Dimensionality reduction}$: Similar to PCA but capable of capturing non-linear structures,\n",
    "- $\\textbf{Feature learning}$: For pre-training deep neural networks,\n",
    "- $\\textbf{Collaborative filtering}$: Used in recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
