{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Unsupervised Learning\n",
    "\n",
    "# Part 1: Classification with K-means algorithm\n",
    "\n",
    "The K-means algorithm is a fundamental tool among the unsupervised learning model. Consider a problem with a dataset $\\mathcal{D} = \\left\\{x_i\\right\\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d$ with no labels, we are aiming at finding some hidden structure within the data, namely, we would like to find clusters in the dataset. Classifiers have been studied in TP but mainly for supervised learning, here the data are not labeled. \n",
    "\n",
    "The K-means algorithm tries to classify the dataset in $K$ clusters. Each cluster is represented by a centroid, meaning the average of the points within the cluster. We note $C_k$ the set of points of a cluster $k$ and $\\mu_k$ its centroid. Then, the algorithm minimizes the intra-cluster variance, in other words, it tries to reduce the distance between the points of the cluster and the centroid. \n",
    "\n",
    "More technically, the algorithm works iteratively in two main steps: \n",
    " - Points are assigned to clusters based on their proximity to existing centroid\n",
    " - Centroids are updated by taking the average of the points assigned to each cluster.\n",
    "\n",
    "\n",
    "\n",
    "The Loss function of the problem can be written as:\n",
    "$$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 \\; ,$$\n",
    "where $\\mu(i)$ is the centroid of the cluster assignated to $x_i$. \n",
    "\n",
    "We want to check the understanding of the choice of this loss function. Does it match the aforementioned rules? Then, could it help to understand if the algorithm converges?\n",
    "\n",
    "Additionnally, until Question 8, we assume that the clusters $C_k$ are disjoint, especially at initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "First, let us familiarize ourselves with the loss function:\n",
    "\n",
    " - Prove the second equality:\n",
    "\n",
    "   $$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$$\n",
    "\n",
    " - What does the term $\\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$ represent?\n",
    " \n",
    " - Explain why this form of the loss function is more convenient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 1: K-means loss decomposition\n",
    "\n",
    "- Equality: clusters {C_k} partition the data and each point x_i is paired with its cluster centroid $\\mu(i)=\\mu_k$. Grouping the point-wise sum by cluster merely reorders terms, giving $J=\\sum_i \\|x_i-\\mu(i)\\|^2 = \\sum_{k=1}^K \\sum_{x_i\\in C_k} \\|x_i-\\mu_k\\|^2$.\n",
    "- Meaning: $\\sum_{x_i\\in C_k}\\|x_i-\\mu_k\\|^2$ is the within-cluster sum of squared distances (dispersion/variance) for cluster $k$.\n",
    "- Convenience: the loss separates cluster-wise; centroid updates and assignment changes affect only their cluster term, making the optimization modular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Let us focus on the first point of the algorihtm, let us consider a single point $x_i$ and add the time dependency. Also, we denote by a $'$ the variables after the new assignement of the data points. \n",
    "\n",
    "So that, the variables are denoted by: $(\\cdot)^t \\to (\\cdot)^t\\,{}' \\to (\\cdot)^{t+1} \\to (\\cdot)^{t+1}\\,{}' \\to (\\cdot)^{t+2}$\n",
    "\n",
    "\n",
    "Thus, at each step time $t$, the new assignements of the variables leads to: (no proof required)\n",
    "\n",
    "$$ \\mu^t(i)' = {\\rm argmin}_{\\mu \\in \\left\\{\\mu^t_k\\right\\}_k} \\lVert x_i - \\mu \\rVert^2$$\n",
    "\n",
    "For instance, at time $t$ before new assignements, the vector $x_i$ belongs to a cluster $k$, while after the next assignement, it now belongs to the cluster $k'$ (it can be the same or different from the cluster $k$). \n",
    "\n",
    "\n",
    "Thus, for the whole dataset, the algorithm updates the assignement as: $\\left\\{\\mu^t(i) \\right\\} \\to \\left\\{\\mu^t(i)'\\right\\}$.\n",
    "\n",
    "\n",
    "Compare $\\lVert x_i - \\mu^t(i) \\rVert^2$ and $\\lVert x_i - \\mu^t(i)' \\rVert^2$ for a given point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 2: Assignment step effect\n",
    "\n",
    "By definition of the reassignment step, $\\mu^t(i)'$ is the closest centroid (among $\\{\\mu_k^t\\}\\_k$) to $x_i$. Hence $\\|x_i - \\mu^t(i)'\\|^2 \\le \\|x_i - \\mu^t(i)\\|^2$.\n",
    "- If $x_i$ stays in the same cluster or ties, the distances are equal.\n",
    "- If $x_i$ moves to a new cluster, the distance strictly decreases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "We recall that we denote by a $'$ the variables after the new assignement of the data points, so that: $\\mu^t(i) \\to \\mu^t(i)'$ and $J_t \\to J_t'$\n",
    "\n",
    "Thanks to the previous question, compare $J_t$ and ${J_t}'$. \n",
    "\n",
    "Hint: Pick the right formula between the two given for $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 3: Effect on $J$ after reassignment\n",
    "\n",
    "- Definitions: $J_t = \\sum_i \\|x_i - \\mu^t(i)\\|^2$ and $J_t' = \\sum_i \\|x_i - \\mu^t(i)'\\|^2$.\n",
    "- Point-wise comparison: by construction of the assignment step, $\\mu^t(i)'$ is the closest centroid among $\\{\\mu_k^t\\}$. Therefore $\\|x_i - \\mu^t(i)'\\|^2 \\le \\|x_i - \\mu^t(i)\\|^2$ for every $i$.\n",
    "- Summing over all points: $J_t' = \\sum_i \\|x_i - \\mu^t(i)'\\|^2 \\le \\sum_i \\|x_i - \\mu^t(i)\\|^2 = J_t$.\n",
    "- Equality vs strict decrease: equality holds if no point changes to a strictly closer centroid (e.g., ties or unchanged clusters); otherwise $J_t' < J_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 4: Cluster-wise optimization setup\n",
    "\n",
    "Goal: minimize $J_t'(\\{\\mu_k\\}_k) = \\sum_{k=1}^K \\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu_k\\|^2$ over centroids $\\{\\mu_k\\}_k$ (assignments fixed).\n",
    "Key observation: the sum is separable by cluster because $\\mu_k$ only appears in the term for $C_k^t{}'$.\n",
    "Rewriting: $J_t' = \\sum_{k=1}^K f_k(\\mu_k)$ with $f_k(\\mu_k) = \\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu_k\\|^2$.\n",
    "Conclusion: we can minimize each $f_k$ independently, yielding $\\mu_k^{t+1} = \\operatorname{argmin}_{\\mu_k} f_k(\\mu_k)$ for every $k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead by studying the second point of the algorithm. It wants to minimize the intra-cluster variance:\n",
    "\n",
    "$$ \\left\\{\\mu^{t+1}_k\\right\\}_k = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} J'_t\\left( \\left\\{\\mu_k\\right\\}_k \\right) = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} \\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$\n",
    "\n",
    "\n",
    "Show that the optimization can be done cluster-wise:\n",
    "\n",
    "$$\\mu^{t+1}_k = {\\rm argmin}_{\\mu_k \\in \\mathbb{R}^d}  \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 5\n",
    "\n",
    "Show that the new centroids of time $t+1$ are computed according to the following equality:\n",
    "\n",
    "$$ \\mu_k^{t+1} = \\frac{1}{|C^t_k{}'|}\\sum_{x_i \\in C^t_k{}'} x_i $$\n",
    "\n",
    "Does it correspond to what you expected from the algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 5: Centroid update\n",
    "\n",
    "We minimize $f_k(\\mu_k) = \\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu_k\\|^2$ for a fixed cluster $C_k^t{}'$.\n",
    "- Expand/gradient: $f_k(\\mu_k) = \\sum_i (x_i^{\\top} x_i - 2 x_i^{\\top} \\mu_k + \\mu_k^{\\top} \\mu_k)$. Gradient: $\\nabla_{\\mu_k} f_k = 2 |C_k^t{}'|\\, \\mu_k - 2 \\sum_{x_i \\in C_k^t{}'} x_i$.\n",
    "- Set to zero: $\\mu_k^{t+1} = \\frac{1}{|C_k^t{}'|} \\sum_{x_i \\in C_k^t{}'} x_i$.\n",
    "- Interpretation: the optimal centroid is the arithmetic mean of its assigned points - exactly the usual K-means update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "\n",
    "If we focus on a cluster $k$ at time $t$ after the assignement, noted $C^t_k {}'$, could you compare $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^t \\rVert^2$ and $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^{t+1} \\rVert^2$ ?\n",
    "\n",
    "What can you say about ${J_t}'$ and $J_{t+1}$ ? Hint: Use the right formula between the two given for $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 6: Improvement from centroid update\n",
    "\n",
    "Consider a fixed cluster $C_k^t{}'$ after the assignment step.\n",
    "- Objective per cluster: $f_k(\\mu) = \\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu\\|^2$. By definition, $\\mu_k^{t+1}$ minimizes $f_k$.\n",
    "- Comparison: $f_k(\\mu_k^{t+1}) \\le f_k(\\mu_k^t)$, i.e., $\\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu_k^{t+1}\\|^2 \\le \\sum_{x_i \\in C_k^t{}'} \\|x_i - \\mu_k^t\\|^2$, with equality only if $\\mu_k^t$ was already the minimizer.\n",
    "- Summing over clusters (using the cluster-wise form of $J$): $J_{t+1} = \\sum_k f_k(\\mu_k^{t+1}) \\le \\sum_k f_k(\\mu_k^t) = J_t'$.\n",
    "Thus the centroid update step does not increase the loss; it strictly decreases it unless each old centroid was already the cluster mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7:\n",
    "\n",
    "Putting together the Questions 3 and 5, compare $J_t$ and $J_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 7: Overall decrease across steps\n",
    "\n",
    "- From Q3 (assignment step): $J_t' \\le J_t$, with strict decrease if any point switches to a closer centroid.\n",
    "- From Q5 (centroid minimizer): choosing $\\mu_k^{t+1}$ as the mean of $C_k^t{}'$ minimizes each cluster term, so $J_{t+1} = \\sum_k \\sum_{x_i\\in C_k^t{}'} \\|x_i - \\mu_k^{t+1}\\|^2 \\le J_t'$, with equality only if the old centroids were already the means.\n",
    "- Chain the inequalities: $J_{t+1} \\le J_t' \\le J_t$; each full iteration is non-increasing and strictly decreases $J$ unless neither step changes anything.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8:\n",
    "\n",
    "After recalling a trivial lower bound for the sequence $(J_t)_{t \\geq 0}$, what can you say about the convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 8: Convergence of $J_t$\n",
    "\n",
    "- Lower bound: $J_t = \\sum_i \\|x_i - \\mu^t(i)\\|^2 \\ge 0$ for all $t$.\n",
    "- Monotone: from previous steps, $J_{t+1} \\le J_t$ (non-increasing sequence).\n",
    "- Convergence: a non-increasing sequence bounded below converges, so $J_t \to J^*$ for some $J^* \\ge 0$.\n",
    "- Note on optimality: the limit corresponds to a fixed point of assignments/centroids (a local minimum or stationary point), not necessarily the global minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 9:\n",
    "\n",
    "We just proved that the algorithm converges, but what about its stability:\n",
    "\n",
    "Let us suppose that the data are sampled from a mixture of $K$ Gaussian, where the choice of $K$ is free for this question. Do you imagine a situation where the algorithm does not classify the data at all? Please design and explain the situation as clearly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 9: Example of instability\n",
    "\n",
    "A mixture of Gaussians can produce bad K-means results if clusters overlap heavily or are anisotropic. One clear failure mode: two components that are symmetric and well-separated in one direction but identical along another, leading to multiple equally good partitions.\n",
    "- Example setup: take two Gaussians in $\\mathbb{R}^2$: means $(+m, 0)$ and $(-m, 0)$, same isotropic covariance $\\sigma^2 I$, equal weights. If $m$ is small relative to $\\sigma$, the blobs overlap strongly, and many initializations give different labelings with similar $J$.\n",
    "- Failure to classify: K-means may converge to centroids both sitting near the global mean (collapsed solution) or swap labels arbitrarily; assignments fluctuate with small perturbations, so the algorithm does not recover the true mixture separation.\n",
    "- More severe: with $K$ larger than the true number of modes, centroids can split a single Gaussian into multiple parts instead of separating true components, producing meaningless clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10:\n",
    "\n",
    "What can you say about those configurations of centroids? What does it imply concerning the minima? Conclude your arguments by discussing the convexity of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 10: Minima and non-convexity\n",
    "\n",
    "- Multiple minima: Different centroid configurations can yield the same or similar $J$ (e.g., centroid label swaps; centroids capturing different symmetric partitions). These are distinct local minima.\n",
    "- Saddle/plateaus: With overlapping or symmetric data, centroids can sit at unstable or flat regions (e.g., both centroids at the global mean), so small perturbations change assignments.\n",
    "- Implication: the K-means objective is non-convex in the joint space of assignments and centroids; optimization can get stuck in local minima depending on initialization.\n",
    "- Practical takeaway: multiple restarts or better initializations (e.g., k-means++) are used to escape poor local minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11:\n",
    "\n",
    "We can also quickly generalize our algorithm.\n",
    "\n",
    "In some situation, you are aiming at favoring some directions in your data and penalizing the others, so that you can weigh the euclidean distance according to:\n",
    "\n",
    "$$d^{(w)}(x_{i}, \\mu(i)) = \\frac{\\sum_{j=1}^d w_j(x_{ij} - \\mu(i)_j)^2}{\\sum_{j=1}^d w_j} $$\n",
    "\n",
    "Show that with a change of variables, the problem remains the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 11\n",
    "\n",
    "We show that a simple linear change of variables turns the **weighted** distance into an ordinary **Euclidean** distance.\n",
    "\n",
    "1. **Define a scaling matrix**\n",
    "\n",
    "   Let  \n",
    "   $$\n",
    "   D = \\operatorname{diag}\\bigl(\\sqrt{w_1}, \\dots, \\sqrt{w_d}\\bigr),\n",
    "   $$\n",
    "   so that multiplying by \\(D\\) scales each coordinate \\(j\\) by \\(\\sqrt{w_j}\\).\n",
    "\n",
    "2. **Change variables**\n",
    "\n",
    "   For each data point \\(x_i \\in \\mathbb{R}^d\\) and its corresponding centroid \\(\\mu(i)\\), define\n",
    "   $$\n",
    "   \\tilde{x}_i = D x_i, \n",
    "   \\qquad\n",
    "   \\tilde{\\mu}(i) = D \\mu(i).\n",
    "   $$\n",
    "\n",
    "3. **Rewrite the numerator**\n",
    "\n",
    "   Consider the numerator of \\(d^{(w)}\\):\n",
    "   $$\n",
    "   \\sum_{j=1}^d w_j \\bigl(x_{ij} - \\mu(i)_j\\bigr)^2\n",
    "   = \\sum_{j=1}^d \\bigl(\\sqrt{w_j}\\,(x_{ij} - \\mu(i)_j)\\bigr)^2.\n",
    "   $$\n",
    "   By construction of \\(D\\),\n",
    "   $$\n",
    "   \\sqrt{w_j}\\,(x_{ij} - \\mu(i)_j)\n",
    "   \\quad\\text{is exactly the } j\\text{-th coordinate of } (\\tilde{x}_i - \\tilde{\\mu}(i)),\n",
    "   $$\n",
    "   hence\n",
    "   $$\n",
    "   \\sum_{j=1}^d w_j \\bigl(x_{ij} - \\mu(i)_j\\bigr)^2\n",
    "   = \\|\\tilde{x}_i - \\tilde{\\mu}(i)\\|^2.\n",
    "   $$\n",
    "\n",
    "4. **Plug back into the weighted distance**\n",
    "\n",
    "   Therefore,\n",
    "   $$\n",
    "   d^{(w)}(x_i, \\mu(i))\n",
    "   = \\frac{1}{\\sum_{j=1}^d w_j}\\,\\|\\tilde{x}_i - \\tilde{\\mu}(i)\\|^2.\n",
    "   $$\n",
    "\n",
    "5. **Conclusion (equivalence of problems)**\n",
    "\n",
    "   Up to the constant factor \\(\\frac{1}{\\sum_{j=1}^d w_j}\\), the weighted distance \\(d^{(w)}\\) is just the **squared Euclidean distance** in the scaled space \\(\\tilde{x}\\).\n",
    "\n",
    "   Hence, minimizing the **weighted** K-means objective over \\((x_i, \\mu_k)\\) is equivalent to running **ordinary** K-means on the transformed points \\((\\tilde{x}_i)\\) with centroids \\((\\tilde{\\mu}_k)\\), and then mapping the centroids back via\n",
    "   $$\n",
    "   \\mu_k = D^{-1} \\tilde{\\mu}_k.\n",
    "   $$\n",
    "\n",
    "   So, after this change of variables, the structure of the K-means problem remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 11\n",
    "\n",
    "We show that a simple linear change of variables turns the **weighted** distance into an ordinary **Euclidean** distance.\n",
    "\n",
    "1. **Define a scaling matrix**\n",
    "\n",
    "   Let  \n",
    "   $$\n",
    "   D = \\operatorname{diag}\\bigl(\\sqrt{w_1}, \\dots, \\sqrt{w_d}\\bigr),\n",
    "   $$\n",
    "   so that multiplying by $D$ scales each coordinate $j$ by $\\sqrt{w_j}$.\n",
    "\n",
    "2. **Change variables**\n",
    "\n",
    "   For each data point $x_i \\in \\mathbb{R}^d$ and its corresponding centroid $\\mu(i)$, define\n",
    "   $$\n",
    "   \\tilde{x}_i = D x_i, \n",
    "   \\qquad\n",
    "   \\tilde{\\mu}(i) = D \\mu(i).\n",
    "   $$\n",
    "\n",
    "3. **Rewrite the numerator**\n",
    "\n",
    "   Consider the numerator of $d^{(w)}$:\n",
    "   $$\n",
    "   \\sum_{j=1}^d w_j \\bigl(x_{ij} - \\mu(i)_j\\bigr)^2\n",
    "   = \\sum_{j=1}^d \\bigl(\\sqrt{w_j}\\,(x_{ij} - \\mu(i)_j)\\bigr)^2.\n",
    "   $$\n",
    "   By construction of $D$,\n",
    "   $$\n",
    "   \\sqrt{w_j}\\,(x_{ij} - \\mu(i)_j)\n",
    "   $$\n",
    "   is exactly the $j$-th coordinate of $(\\tilde{x}_i - \\tilde{\\mu}(i))$, hence\n",
    "   $$\n",
    "   \\sum_{j=1}^d w_j \\bigl(x_{ij} - \\mu(i)_j\\bigr)^2\n",
    "   = \\|\\tilde{x}_i - \\tilde{\\mu}(i)\\|^2.\n",
    "   $$\n",
    "\n",
    "4. **Plug back into the weighted distance**\n",
    "\n",
    "   Therefore,\n",
    "   $$\n",
    "   d^{(w)}(x_i, \\mu(i))\n",
    "   = \\frac{1}{\\sum_{j=1}^d w_j}\\,\\|\\tilde{x}_i - \\tilde{\\mu}(i)\\|^2.\n",
    "   $$\n",
    "\n",
    "5. **Conclusion**\n",
    "\n",
    "   Up to the constant factor $\\frac{1}{\\sum_{j=1}^d w_j}$, the weighted distance $d^{(w)}$ is just the **squared Euclidean distance** in the scaled space $\\tilde{x}$.\n",
    "\n",
    "   Hence, minimizing the **weighted** K-means objective over $(x_i, \\mu_k)$ is equivalent to running **ordinary** K-means on the transformed points $(\\tilde{x}_i)$ with centroids $(\\tilde{\\mu}_k)$, and then mapping the centroids back via\n",
    "   $$\n",
    "   \\mu_k = D^{-1} \\tilde{\\mu}_k.\n",
    "   $$\n",
    "\n",
    "   So, after this change of variables, the structure of the K-means problem remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Restricted Boltzmann Machine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The Boltzmann Machine have been inspired by thermodynamic and statistical physics models, more precisely they are part of the Energy Models using the well known Boltzmann Distribution as written in physics style:\n",
    "\n",
    "$$ P\\left( E \\right)  = \\frac{1}{Z} \\exp \\left( -\\frac{E}{k_b T} \\right)$$\n",
    "\n",
    "It becomes in statistical inference framework:\n",
    "$$\n",
    "P(\\mathbf{v} | J, \\mathbf{b}) \\propto e^{\\mathbf{v}^TJ\\mathbf{v} + \\mathbf{b}^T\\mathbf{v}} = e^{-E(\\mathbf{v})}\n",
    "$$\n",
    "where:\n",
    "- $\\mathbf{v}\\in\\mathbb{R}^n:$ The binary vector with components $v_i = 0 \\; {\\rm or} \\; 1$\n",
    "\n",
    "- $J \\in \\mathbb{R}^{n \\times n}:$ The coupling matrix\n",
    "\n",
    "- $\\mathbf{b} \\in  \\mathbb{R}^n$: Field\n",
    "\n",
    "- $E(\\mathbf{v}) \\in  \\mathbb{R}$: Energy\n",
    "\n",
    "\n",
    "However, one problem arised with initial Boltzmann Machine (BM) -- like its parent models in statistical physics (as the SK model) -- all the units are interacting through complicated dependencies. For example, if we consider 3 components of $\\mathbf{v}$: $v_1$, $v_2$, and $v_3$, there are trivial interactions such as one modelised by $P(v_1, v_2)$ corresponding to the correlation between the two first components of $\\mathbf{v}$, but there are also none trivial interactions. Indeed, if some term like $P(v_1, v_2 | v_3)$ which suggests that the correlation $x_1$ and $v_2$ depends on $v_3$ and this is clearly none linear.\n",
    "\n",
    "A really ingenious way to overcome this situation is to replace all the tricky interactions between the units $\\mathbf{v}\\in\\mathbb{R}^n$ by connections through hidden units $\\mathbf{h}\\in\\mathbb{R}^m$, artifically created. Indeed, correlations between two units $v_1$ and $v_2$ (specially the dependency of their correlations on other units $v_3$, $v_4$,...) can be atrificially replaced by introducing a third unit $h_1$ and considerin only linear correlations between $v_1 \\leftrightarrow h_1$, $h_1 \\leftrightarrow v_2$ and $v_1 \\leftrightarrow v_2$. The units $v_i$ are now called the visible units. This model is the most known version of BMs. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"boltzmannmachine.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "However, this model is still fully connected and makes the computation really costful. Then, one can even simplify the model by considering zero intra layer interractions. This simplified model is call Restricted Boltzmann Machine (RBM) (Physics Nobel Price 2024 ü•≥).\n",
    "\n",
    "Thus, the RBM architecture consists of two layers of binary stochastic units: a $\\textbf{visible layer}$ $\\mathbf{v}$ and a $\\textbf{hidden layer}$ $\\mathbf{h}$. The layers are fully connected, but there are no connections within a layer, making the model a $\\textbf{bipartite graph}$. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"rbm.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "Restricted Boltzmann Machines (RBMs) are a class of energy-based probabilistic graphical models that are commonly used in machine learning for tasks such as dimensionality reduction, feature learning, and generative modeling.\n",
    "\n",
    "### Energy Function and Probabilities\n",
    "\n",
    "The joint configuration of the visible units $\\mathbf{v} \\in \\{0, 1\\}^d$ and the hidden units $\\mathbf{h} \\in \\{0, 1\\}^m$ is associated with an $\\textbf{energy function}$, defined as:\n",
    "\n",
    "$$ E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{v}^\\top \\mathbf{W} \\mathbf{h} - \\mathbf{b}^\\top \\mathbf{v} - \\mathbf{c}^\\top \\mathbf{h}$$\n",
    "where:\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{d \\times m}$ is the weight matrix connecting the visible and hidden units,\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^d$ field of the visible units or also called the biases of the visible units,\n",
    "- $\\mathbf{c} \\in \\mathbb{R}^m$ field of the hidden units of also called the biases of the hidden units.\n",
    "\n",
    "The energy function determines the joint probability distribution over $\\mathbf{v}$ and $\\mathbf{h}$:\n",
    "$$ P(\\mathbf{v}, \\mathbf{h}) = \\frac{1}{Z} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "where $Z$ is the $\\textbf{partition function}$, ensuring normalization:\n",
    "\n",
    "$$ Z = \\sum_{\\mathbf{v}, \\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "\n",
    "\n",
    "The marginal probability of the visible units $\\mathbf{v}$ is obtained by summing over all possible configurations of the hidden units:\n",
    "\n",
    "$$ P(\\mathbf{v}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12:\n",
    "\n",
    "Write a valid expression of the energy $E(\\textbf{v}, \\textbf{h})$ in the case of a BM (non-restricted) with an hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 12: Energy of a general Boltzmann Machine with a hidden layer\n",
    "\n",
    "Variables:\n",
    "- Visible units: $v \\in \\{0,1\\}^d$\n",
    "- Hidden units: $h \\in \\{0,1\\}^m$\n",
    "\n",
    "Parameters:\n",
    "- Visible biases: $b \\in \\mathbb{R}^d$\n",
    "- Hidden biases: $c \\in \\mathbb{R}^m$\n",
    "- Visible‚Äìvisible weights: $L \\in \\mathbb{R}^{d \\times d}$, symmetric with zero diagonal\n",
    "- Hidden‚Äìhidden weights: $K \\in \\mathbb{R}^{m \\times m}$, symmetric with zero diagonal\n",
    "- Visible‚Äìhidden weights: $W \\in \\mathbb{R}^{d \\times m}$\n",
    "\n",
    "A valid energy:\n",
    "$$\n",
    "E(v,h) = -\\,b^\\top v \\;-\\; c^\\top h \\;-\\; \\tfrac{1}{2}\\,v^\\top L\\,v \\;-\\; v^\\top W\\,h \\;-\\; \\tfrac{1}{2}\\,h^\\top K\\,h \\, .\n",
    "$$\n",
    "The $\\tfrac{1}{2}$ factors avoid double counting symmetric pairs in $L$ and $K$ and the zero diagonals avoid self-interactions.\n",
    "\n",
    "Block-matrix form:\n",
    "Define $x = \\begin{bmatrix} v \\\\ h \\end{bmatrix}$ and $M = \\begin{bmatrix} L & W \\\\ W^\\top & K \\end{bmatrix}$ (symmetric, zero diagonal). Then\n",
    "$$\n",
    "E(x) = -\\,\\tfrac{1}{2}\\,x^\\top M x \\;-\\; \\begin{bmatrix} b \\\\ c \\end{bmatrix}^\\top x \\, .\n",
    "$$\n",
    "Either expression is a correct energy for a non-restricted Boltzmann Machine with one hidden layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13:\n",
    "\n",
    "One of the key properties of RBMs is the $\\textbf{conditional independence}$ between units within a layer:\n",
    "\n",
    "Compute the conditional probability and show that:\n",
    "\n",
    "$$ P(h_j = 1 | \\mathbf{v}) = \\sigma\\left(c_j + \\sum_{i} v_i W_{ij}\\right) $$\n",
    "and\n",
    "$$ P(v_i = 1 | \\mathbf{h}) = \\sigma\\left(b_i + \\sum_{j} h_j W_{ij}\\right) $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid activation function.\n",
    "\n",
    "This bipartite structure enables efficient Gibbs sampling for approximating the intractable joint distribution $P(\\mathbf{v}, \\mathbf{h})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 13:\n",
    "\n",
    "Setup:\n",
    "- Binary visible units: $v \\in \\{0,1\\}^d$\n",
    "- Binary hidden units: $h \\in \\{0,1\\}^m$\n",
    "- Parameters: weights $W \\in \\mathbb{R}^{d \\times m}$, visible biases $b \\in \\mathbb{R}^d$, hidden biases $c \\in \\mathbb{R}^m$\n",
    "- RBM energy: $E(v,h) = -\\,v^\\top W h \\;-\\; b^\\top v \\;-\\; c^\\top h$\n",
    "\n",
    "Key property:\n",
    "- There are no visible‚Äìvisible or hidden‚Äìhidden edges. Conditioned on one layer, units in the other layer become independent.\n",
    "\n",
    "1) Conditional of hidden given visible\n",
    "Starting from the joint $P(v,h) \\propto \\exp(-E(v,h))$, fix $v$:\n",
    "$$\n",
    "P(h \\mid v) \\propto \\exp\\big((W^\\top v + c)^\\top h\\big) = \\prod_{j=1}^m \\exp\\big(h_j (c_j + (W^\\top v)_j)\\big).\n",
    "$$\n",
    "Since $h_j \\in \\{0,1\\}$, each factor is a Bernoulli:\n",
    "$$\n",
    "P(h_j = 1 \\mid v) = \\sigma\\!\\left(c_j + \\sum_i v_i W_{ij}\\right), \\quad P(h_j = 0 \\mid v) = 1 - P(h_j=1 \\mid v),\n",
    "$$\n",
    "and the $h_j$ are conditionally independent given $v$.\n",
    "\n",
    "2) Conditional of visible given hidden\n",
    "Similarly, fix $h$:\n",
    "$$\n",
    "P(v \\mid h) \\propto \\exp\\big((W h + b)^\\top v\\big) = \\prod_{i=1}^d \\exp\\big(v_i (b_i + (W h)_i)\\big).\n",
    "$$\n",
    "Each $v_i$ is Bernoulli:\n",
    "$$\n",
    "P(v_i = 1 \\mid h) = \\sigma\\!\\left(b_i + \\sum_j h_j W_{ij}\\right), \\quad P(v_i = 0 \\mid h) = 1 - P(v_i=1 \\mid h),\n",
    "$$\n",
    "and the $v_i$ are conditionally independent given $h$.\n",
    "\n",
    "Sigmoid:\n",
    "$\\sigma(x) = 1/(1+e^{-x})$.\n",
    "\n",
    "Conclusion:\n",
    "The bipartite structure yields conditional independence within each layer and closed-form Bernoulli conditionals with sigmoid parameters, enabling efficient block Gibbs sampling between $v$ and $h$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in RBMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14:\n",
    "\n",
    "Training an RBM involves maximizing the likelihood of the data distribution. To do so we are aiming at using a gradient descent/ascent on the weights (and biases).\n",
    "\n",
    "Compute the log-likelihood $\\mathcal{L}(\\mathbf{v})$, remember that the model is part of the unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 14: Log-likelihood of an RBM\n",
    "\n",
    "Setup:\n",
    "- Visible binary vector: $v \\in \\{0,1\\}^d$\n",
    "- Hidden binary vector: $h \\in \\{0,1\\}^m$\n",
    "- Parameters: weights $W \\in \\mathbb{R}^{d \\times m}$, biases $b \\in \\mathbb{R}^d$ (visible), $c \\in \\mathbb{R}^m$ (hidden)\n",
    "- Energy: $E(v,h) = -\\,v^\\top W h \\;-\\; b^\\top v \\;-\\; c^\\top h$\n",
    "\n",
    "1) Joint and marginal\n",
    "The joint is $P(v,h) = \\frac{1}{Z} \\exp(-E(v,h))$ with partition function $Z = \\sum_{v,h} \\exp(-E(v,h))$.\n",
    "The marginal over $v$ is $p(v) = \\sum_h P(v,h) = \\frac{1}{Z} \\sum_h \\exp(-E(v,h))$.\n",
    "\n",
    "2) Factor the sum over $h$\n",
    "Plugging $E(v,h)$:\n",
    "$$\n",
    "p(v) = \\frac{1}{Z} \\exp(b^\\top v) \\sum_h \\exp\\big( h^\\top (W^\\top v + c) \\big)\n",
    "     = \\frac{1}{Z} \\exp(b^\\top v) \\prod_{j=1}^m \\sum_{h_j \\in \\{0,1\\}} \\exp\\big( h_j (c_j + (W^\\top v)_j) \\big).\n",
    "$$\n",
    "Each inner sum is $1 + \\exp(c_j + (W^\\top v)_j)$.\n",
    "\n",
    "3) Closed form for $p(v)$\n",
    "$$\n",
    "p(v) = \\frac{1}{Z} \\exp(b^\\top v) \\prod_{j=1}^m \\big(1 + e^{\\,c_j + (W^\\top v)_j}\\big).\n",
    "$$\n",
    "\n",
    "4) Log-likelihood\n",
    "$$\n",
    "\\log p(v) = b^\\top v \\;+\\; \\sum_{j=1}^m \\log\\big(1 + e^{\\,c_j + \\sum_i v_i W_{ij}}\\big) \\;-\\; \\log Z.\n",
    "$$\n",
    "\n",
    "5) Free-energy form (optional)\n",
    "Define the free energy $F(v) = -\\,b^\\top v \\;-\\; \\sum_j \\log\\big(1 + e^{\\,c_j + (W^\\top v)_j}\\big)$.\n",
    "Then $\\log p(v) = -F(v) - \\log Z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15:\n",
    "\n",
    "Compute the gradient of the log-likelihood with respect to the weights $\\mathbf{W}$ and the biases $\\mathbf{b}$, $\\mathbf{c}$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it should be possible to implement the RBM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 15: Gradients of the RBM log-likelihood\n",
    "\n",
    "Setup\n",
    "- Data: visible vectors $v^{(n)} \\in \\{0,1\\}^d$, $n=1,\\dots,N$.\n",
    "- Hidden: $h \\in \\{0,1\\}^m$.\n",
    "- Parameters: $W \\in \\mathbb{R}^{d \\times m}$, $b \\in \\mathbb{R}^d$ (visible biases), $c \\in \\mathbb{R}^m$ (hidden biases).\n",
    "- Energy: $E(v,h) = -\\,v^\\top W h \\;-\\; b^\\top v \\;-\\; c^\\top h$.\n",
    "- Likelihood (per datum $v$): $p(v) = \\frac{1}{Z} \\sum_h e^{-E(v,h)}$, with $Z = \\sum_{v,h} e^{-E(v,h)}$.\n",
    "- Log-likelihood over the dataset: $\\mathcal{L} = \\sum_{n=1}^N \\log p(v^{(n)})$.\n",
    "\n",
    "General pattern\n",
    "For any parameter $\\theta \\in \\{W_{ij}, b_i, c_j\\}$,\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
    "= \\sum_{n=1}^N \\underbrace{\\mathbb{E}_{p(h\\mid v^{(n)})}\\big[-\\partial_\\theta E(v^{(n)},h)\\big]}_{\\text{data (positive phase)}}\n",
    "\\;-\\; N \\underbrace{\\mathbb{E}_{p(v,h)}\\big[-\\partial_\\theta E(v,h)\\big]}_{\\text{model (negative phase)}}.\n",
    "$$\n",
    "\n",
    "Gradients w.r.t. $W_{ij}$\n",
    "- Energy derivative: $\\partial E / \\partial W_{ij} = -\\,v_i h_j$.\n",
    "- Data term (for datum $v^{(n)}$): $\\mathbb{E}_{p(h\\mid v^{(n)})}[v_i^{(n)} h_j] = v_i^{(n)} \\,\\mathbb{E}_{p(h\\mid v^{(n)})}[h_j]$.\n",
    "- Model term: $\\mathbb{E}_{p(v,h)}[v_i h_j]$.\n",
    "\n",
    "Putting it together:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{ij}}\n",
    "= \\sum_{n=1}^N v_i^{(n)} \\,\\mathbb{E}_{p(h\\mid v^{(n)})}[h_j]\n",
    "\\;-\\; N \\,\\mathbb{E}_{p(v,h)}[v_i h_j].\n",
    "$$\n",
    "\n",
    "In matrix form (averaged over data):\n",
    "$$\n",
    "\\nabla_W \\mathcal{L}\n",
    "= \\Big\\langle v\\, h^\\top \\Big\\rangle_{\\text{data}}\n",
    "\\;-\\; \\Big\\langle v\\, h^\\top \\Big\\rangle_{\\text{model}},\n",
    "$$\n",
    "where $\\langle \\cdot \\rangle_{\\text{data}}$ is expectation under $p(h\\mid v^{(n)})$ with $v^{(n)}$ from the dataset, and $\\langle \\cdot \\rangle_{\\text{model}}$ is under the model joint $p(v,h)$.\n",
    "\n",
    "Gradients w.r.t. visible biases $b_i$\n",
    "- Energy derivative: $\\partial E / \\partial b_i = -\\,v_i$.\n",
    "- Data term: $\\mathbb{E}_{p(h\\mid v^{(n)})}[v_i^{(n)}] = v_i^{(n)}$.\n",
    "- Model term: $\\mathbb{E}_{p(v,h)}[v_i]$.\n",
    "\n",
    "Result:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_i}\n",
    "= \\sum_{n=1}^N v_i^{(n)} \\;-\\; N\\, \\mathbb{E}_{p(v,h)}[v_i].\n",
    "$$\n",
    "\n",
    "Vector form (averaged):\n",
    "$$\n",
    "\\nabla_b \\mathcal{L}\n",
    "= \\big\\langle v \\big\\rangle_{\\text{data}}\n",
    "\\;-\\; \\big\\langle v \\big\\rangle_{\\text{model}}.\n",
    "$$\n",
    "\n",
    "Gradients w.r.t. hidden biases $c_j$\n",
    "- Energy derivative: $\\partial E / \\partial c_j = -\\,h_j$.\n",
    "- Data term: $\\mathbb{E}_{p(h\\mid v^{(n)})}[h_j]$.\n",
    "- Model term: $\\mathbb{E}_{p(v,h)}[h_j]$.\n",
    "\n",
    "Result:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial c_j}\n",
    "= \\sum_{n=1}^N \\mathbb{E}_{p(h\\mid v^{(n)})}[h_j]\n",
    "\\;-\\; N\\, \\mathbb{E}_{p(v,h)}[h_j].\n",
    "$$\n",
    "\n",
    "Vector form (averaged):\n",
    "$$\n",
    "\\nabla_c \\mathcal{L}\n",
    "= \\big\\langle h \\big\\rangle_{\\text{data}}\n",
    "\\;-\\; \\big\\langle h \\big\\rangle_{\\text{model}}.\n",
    "$$\n",
    "\n",
    "Summary\n",
    "- $\\nabla_W \\mathcal{L} = \\langle v h^\\top \\rangle_{\\text{data}} - \\langle v h^\\top \\rangle_{\\text{model}}$\n",
    "- $\\nabla_b \\mathcal{L} = \\langle v \\rangle_{\\text{data}} - \\langle v \\rangle_{\\text{model}}$\n",
    "- $\\nabla_c \\mathcal{L} = \\langle h \\rangle_{\\text{data}} - \\langle h \\rangle_{\\text{model}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16: (Open question)\n",
    "\n",
    "While it seems possible to run RBM algorithm, note that the second term in the gradient w.r.t. $\\mathbf{W}$ is computationally expensive due to the intractability of $Z$, the approximation Contrastive Divergence - k is often use. Research what is this approximation, is this approximation enough, why? Explain it with your own words and cite the papers you used for your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Question 16: Contrastive Divergence (CD‚Äìk) and why it‚Äôs used\n",
    "\n",
    "Goal: Learn RBM parameters by maximizing log-likelihood. Exact gradients need the ‚Äúmodel‚Äù expectations ‚ü®¬∑‚ü©model over p(v,h), which require summing/integrating over all configurations‚Äîthis is intractable for anything but tiny models.\n",
    "\n",
    "Contrastive Divergence idea:\n",
    "- Positive phase: use data samples and exact conditionals p(h|v_data) to get ‚ü®v h^T‚ü©data.\n",
    "- Negative phase: approximate ‚ü®v h^T‚ü©model by running a short Gibbs chain starting from the data (or persistent chain) rather than running a long Markov chain to equilibrium.\n",
    "\n",
    "CD‚Äìk algorithm (for one update):\n",
    "1) Start at a data point v^0 = v_data.\n",
    "2) Sample h^0 ~ p(h | v^0) using the RBM conditional sigmoid.\n",
    "3) For t = 0,‚Ä¶,k‚Äì1:\n",
    "   - Sample v^{t+1} ~ p(v | h^t)\n",
    "   - Sample h^{t+1} ~ p(h | v^{t+1})\n",
    "4) Use v^0,h^0 for the positive phase and v^k,h^k for the negative phase:\n",
    "   - ŒîW ‚àù ‚ü®v^0 (h^0)^T‚ü© ‚Äì ‚ü®v^k (h^k)^T‚ü©\n",
    "   - Œîb ‚àù ‚ü®v^0‚ü© ‚Äì ‚ü®v^k‚ü©\n",
    "   - Œîc ‚àù ‚ü®h^0‚ü© ‚Äì ‚ü®h^k‚ü©\n",
    "\n",
    "Why it‚Äôs used:\n",
    "- Practicality: CD‚Äìk avoids the expensive sampling to equilibrium; k is small (often 1‚Äì10), giving fast, low-variance updates in practice.\n",
    "- Good empirical performance: Despite being a biased estimator of the true gradient (for finite k), CD‚Äìk often learns useful representations and converges to good models in practice.\n",
    "- Simplicity: Easy to implement with alternating Gibbs steps using the RBM‚Äôs closed-form conditionals.\n",
    "\n",
    "Is CD‚Äìk enough?\n",
    "- CD‚Äìk is a biased gradient estimator for finite k; the bias shrinks as k ‚Üí ‚àû (approaching exact maximum likelihood).\n",
    "- In practice, small k (e.g., CD-1 or CD-10) often suffices to learn good filters/features, especially early in training.\n",
    "- For better long-run accuracy/mixing, alternatives like Persistent CD (PCD) keep a running Markov chain (not reinitializing from data each step) to better approximate the model distribution.\n",
    "- For more faithful likelihood training, methods like stochastic maximum likelihood (SML/PCD), tempered transitions, or AIS can be used, but with more compute cost.\n",
    "\n",
    "Key takeaway:\n",
    "CD‚Äìk trades off some bias for big computational savings. It‚Äôs widely used because it is fast, simple, and yields good results in many practical settings, even though it does not produce an exact maximum-likelihood gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of RBMs\n",
    "\n",
    "RBMs are widely used in tasks such as:\n",
    "\n",
    "- $\\textbf{Dimensionality reduction}$: Similar to PCA but capable of capturing non-linear structures,\n",
    "- $\\textbf{Feature learning}$: For pre-training deep neural networks,\n",
    "- $\\textbf{Collaborative filtering}$: Used in recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
